{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-WKVp4Uukcl"
      },
      "source": [
        "#Install Server Requirements\n",
        "This should be run on a minimum of a T4 runtime, though it will run on a CPU only session, however long TTS generations may time out/error.\n",
        "\n",
        "This is a **work in progress**. Known issues:\n",
        "\n",
        "- The 1st TTS generation has a brief stutter.\n",
        "- RVC is not yet working.\n",
        "- Transcoding/ffmpeg isnt working.\n",
        "- Things yet to do on selecting your first model and other configuration setups.\n",
        "\n",
        "If you enable DeepSpeed for XTTS models, DeepSpeed has to compile on the 1st TTS generation which can take about 90 seconds. After that it should be fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6ls-RHTQeZv",
        "outputId": "04e8bd03-b270-42a2-db95-ac65aac390a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*****************************\n",
            "*** Installing DeepSpeed ****\n",
            "*****************************\n",
            "\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.16.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.10.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Collecting nvidia-ml-py (from deepspeed)\n",
            "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.16.4-py3-none-any.whl size=1562653 sha256=298ef101f53b90d6cde2ecc7da9e326bd4478932e976dcaaec62750e6e75dc99\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/6c/e5/ccad75c8ade9cb21e74721affd6d17820b1806249aac34f7f0\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: nvidia-ml-py, hjson, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.16.4 hjson-3.1.0 ninja-1.11.1.3 nvidia-ml-py-12.570.86\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (3.10.15)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n",
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#@markdown Click the `Play button` to the left of this message to install the requirements.<br><br>\n",
        "#@markdown **OPTIONAL** Mounting your Google Drive allows you to drag and drop samples/models via the `drive/Mydrive` path. This allows you<br>\n",
        "#@markdown to store or use specific audio samples or finetuned models.<br>\n",
        "#@markdown **Audio samples** need to be placed in `alltalk_tts/voices`<br>\n",
        "#@markdown **XTTS models** need to be placed in `alltalk_tts/models/xtts/{yourmodelfolderhere}`<br>\n",
        "mount_gdrive = True #@param{type:\"boolean\"}\n",
        "\n",
        "if mount_gdrive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "from IPython.display import clear_output\n",
        "print(\"*******************************************************************\")\n",
        "print(\"** Installing server requirements. This will take 5-10 minutes ****\")\n",
        "print(\"*******************************************************************\")\n",
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 libaio-dev espeak-ng > '/dev/null' 2>&1\n",
        "clear_output()\n",
        "print(\"************************\")\n",
        "print(\"*** Cloning AllTalk ****\")\n",
        "print(\"************************\\n\")\n",
        "!git clone -b alltalkbeta https://github.com/erew123/alltalk_tts\n",
        "clear_output()\n",
        "print(\"\\n********************************\")\n",
        "print(\"*** Installing Requirements ****\")\n",
        "print(\"********************************\\n\")\n",
        "!python -m pip install --upgrade \"pip<24.1\"\n",
        "!pip install --quiet -r /content/alltalk_tts/system/requirements/requirements_colab.txt\n",
        "clear_output()\n",
        "print(\"\\n*****************************\")\n",
        "print(\"*** Installing DeepSpeed ****\")\n",
        "print(\"*****************************\\n\")\n",
        "!pip install deepspeed\n",
        "!pip install orjson\n",
        "!pip install faiss-cpu\n",
        "!pip install fairseq\n",
        "clear_output()\n",
        "print(\"\\n******************************\")\n",
        "print(\"*** Installing Cloudflare ****\")\n",
        "print(\"******************************\\n\")\n",
        "# Install cloudflare\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb > '/dev/null' 2>&1\n",
        "!apt install ./cloudflared-linux-amd64.deb aria2 > '/dev/null' 2>&1\n",
        "!rm cloudflared-linux-amd64.deb > '/dev/null' 2>&1\n",
        "!python -m spacy download en_core_web_md\n",
        "clear_output()\n",
        "print(\"\\n***************************\")\n",
        "print(\"*** Installing Cutlass ****\")\n",
        "print(\"***************************\\n\")\n",
        "# Clone the CUTLASS repository\n",
        "!git clone https://github.com/NVIDIA/cutlass.git\n",
        "!export CUTLASSPATH=/content/cutlass\n",
        "!sudo curl -L https://github.com/BtbN/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-linux64-gpl.tar.xz -o /usr/local/bin/ffmpeg.tar.xz\n",
        "clear_output()\n",
        "%cd /usr/local/bin/\n",
        "clear_output()\n",
        "!7z e /usr/local/bin/ffmpeg.tar.xz -y\n",
        "clear_output()\n",
        "!7z e /usr/local/bin/ffmpeg.tar -y\n",
        "clear_output()\n",
        "!sudo chmod a+rx /usr/local/bin/ffmpeg\n",
        "clear_output()\n",
        "!pip uninstall jax -y\n",
        "clear_output()\n",
        "print(\"************************************\")\n",
        "print(\"** Server requirements installed ***\")\n",
        "print(\"*** Please proceed to next step ****\")\n",
        "print(\"************************************\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nk26r66RQ23"
      },
      "source": [
        "\n",
        "# Start AllTalk TTS Server\n",
        "\n",
        "This will start the AllTalk API and Gradio Web interface. From here you can download models, generate TTS and use external applications via the API address.\n",
        "\n",
        "The AllTalk API address is what you would use in Kobold, SillyTavern, TGWUI's Remote extension etc if you want to generate TTS with those applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9PCQhSoiWvhL"
      },
      "outputs": [],
      "source": [
        "#@markdown Click the `Play button` to the left of this message to start AllTalk API and Gradio Interface<br>\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import threading\n",
        "import time\n",
        "\n",
        "def keep_alive():\n",
        "    while True:\n",
        "        time.sleep(60)  # Run every 60 seconds (adjust as needed)\n",
        "\n",
        "keep_alive_thread = threading.Thread(target=keep_alive)\n",
        "keep_alive_thread.start()\n",
        "\n",
        "Tunnel = \"cloudflare\"\n",
        "host = \"127.0.0.1\"\n",
        "ports = [7851, 7852]\n",
        "tunnel_urls = []\n",
        "\n",
        "# Starting tunnels for each port.\n",
        "for port in ports:\n",
        "    if Tunnel == \"cloudflare\":\n",
        "        !nohup cloudflared tunnel --url http://{host}:{port} > lt_{port}.log 2>&1 &\n",
        "    else:\n",
        "        !nohup npx lt -p {port} > lt_{port}.log 2>&1 &\n",
        "\n",
        "# Wait a couple of seconds for the tunnels to initialize.\n",
        "time.sleep(10)\n",
        "\n",
        "# Extract URLs for each tunnel.\n",
        "for port in ports:\n",
        "    log_file = f'lt_{port}.log'\n",
        "    with open(log_file, 'r') as testwritefile:\n",
        "        log_content = testwritefile.read()\n",
        "\n",
        "        # Use regular expressions to find the URL.\n",
        "        if Tunnel == \"cloudflare\":\n",
        "            url_match = re.search(r'(https://[-a-z0-9]+\\.trycloudflare\\.com)', log_content)\n",
        "        else:\n",
        "            url_match = re.search(r'your url is: (https?://\\S+)', log_content)\n",
        "\n",
        "        if url_match:\n",
        "            tunnel_url = url_match.group(1)\n",
        "            tunnel_urls.append(tunnel_url)\n",
        "        else:\n",
        "            print(f\"URL for port {port} not found.\")\n",
        "\n",
        "# Save the tunnel URLs to a JSON file.\n",
        "try:\n",
        "    # Try to open the JSON file for reading.\n",
        "    with open('/content/alltalk_tts/googlecolab.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    # If the file doesn't exist, create an empty dictionary.\n",
        "    data = {}\n",
        "\n",
        "data['google_ip_address'] = tunnel_urls\n",
        "\n",
        "# Write the modified data (or newly created data) back to the file.\n",
        "with open('/content/alltalk_tts/googlecolab.json', 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "host = \"0.0.0.0\"\n",
        "\n",
        "if Tunnel == \"localtunnel\":\n",
        "    print(\"Before you copy the link above click on it and copy that ip:\")\n",
        "    !curl ipv4.icanhazip.com\n",
        "\n",
        "# Start API server.\n",
        "!python /content/alltalk_tts/script.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start XTTS model Finetuning\n",
        "\n",
        "Starts the Finetuning application for XTTS models.\n",
        "\n",
        "You can either run the `Start AllTalk` to download the base XTTS model(s) for finetuning. Or you can use the folder icon on the left hand side of the screen and upload an XTTS model that you want to finetune. If you are manually uploading a model, you would place your model files in `models/xtts/{yourmodelfolderhere}` and you will need all the models files in that folder `config.json, dvae.pth, mel_stats.pth, model.pth, speakers_xtts.pth, vocab.json`. Without 1x model available, Finetuning will not start.\n",
        "\n",
        "Likewise you can download your finetuned model from there, OR copy it to your Google Drive after finetuning, for later use in AllTalk. To access the Finetuning Gradio interface, connect to the `Google Colab Finetuning url` when Finetuning has started."
      ],
      "metadata": {
        "id": "gDzNPMwM7qvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Click the `Play button` to the left of this message to start Finetuning<br>\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import threading\n",
        "import time\n",
        "\n",
        "def keep_alive():\n",
        "    while True:\n",
        "        time.sleep(60)  # Run every 60 seconds (adjust as needed)\n",
        "\n",
        "keep_alive_thread = threading.Thread(target=keep_alive)\n",
        "keep_alive_thread.start()\n",
        "\n",
        "Tunnel = \"cloudflare\"\n",
        "host = \"127.0.0.1\"\n",
        "ports = [7052]\n",
        "tunnel_urls = []\n",
        "\n",
        "# Starting tunnels for each port.\n",
        "for port in ports:\n",
        "    if Tunnel == \"cloudflare\":\n",
        "        !nohup cloudflared tunnel --url http://{host}:{port} > lt_{port}.log 2>&1 &\n",
        "    else:\n",
        "        !nohup npx lt -p {port} > lt_{port}.log 2>&1 &\n",
        "\n",
        "# Wait a couple of seconds for the tunnels to initialize.\n",
        "time.sleep(10)\n",
        "\n",
        "# Extract URLs for each tunnel.\n",
        "for port in ports:\n",
        "    log_file = f'lt_{port}.log'\n",
        "    with open(log_file, 'r') as testwritefile:\n",
        "        log_content = testwritefile.read()\n",
        "\n",
        "        # Use regular expressions to find the URL.\n",
        "        if Tunnel == \"cloudflare\":\n",
        "            url_match = re.search(r'(https://[-a-z0-9]+\\.trycloudflare\\.com)', log_content)\n",
        "        else:\n",
        "            url_match = re.search(r'your url is: (https?://\\S+)', log_content)\n",
        "\n",
        "        if url_match:\n",
        "            tunnel_url = url_match.group(1)\n",
        "            tunnel_urls.append(tunnel_url)\n",
        "            print(f\"Google Colab Finetuning url: {tunnel_url}\\n\")\n",
        "            print(f\"********************************************************************\")\n",
        "            print(f\"**** Use the above URL to connect to Finetuning on Google Colab ****\")\n",
        "            print(f\"********************************************************************\")\n",
        "            print(f\"************* Now starting the Finetuning Application **************\")\n",
        "            print(f\"********************************************************************\\n\")\n",
        "        else:\n",
        "            print(f\"URL for port {port} not found.\")\n",
        "\n",
        "# Save the tunnel URLs to a JSON file.\n",
        "try:\n",
        "    # Try to open the JSON file for reading.\n",
        "    with open('/content/alltalk_tts/googlecolab.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    # If the file doesn't exist, create an empty dictionary.\n",
        "    data = {}\n",
        "\n",
        "data['google_ip_address'] = tunnel_urls\n",
        "\n",
        "# Write the modified data (or newly created data) back to the file.\n",
        "with open('/content/alltalk_tts/googlecolab.json', 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "host = \"0.0.0.0\"\n",
        "\n",
        "if Tunnel == \"localtunnel\":\n",
        "    print(\"Before you copy the link above click on it and copy that ip:\")\n",
        "    !curl ipv4.icanhazip.com\n",
        "\n",
        "# Start API server.\n",
        "!python /content/alltalk_tts/finetune.py\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Dewvq5s38Sfd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}