{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ijF2_4OOlp8Ob27wiYMrLWA--Ne_oAVY",
      "authorship_tag": "ABX9TyMowBKY9yh0wKSqz1ksJo5v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Boga28/Google-Colab/blob/main/AutoTTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "G1O9iMYhGrwU",
        "outputId": "45ad19c5-056d-4bcf-bd18-d83b45e37c41"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2gVoN1xGDByH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5FYoMd0tYDl"
      },
      "outputs": [],
      "source": [
        "#@markdown Install Library\n",
        "\n",
        "%%bash\n",
        "set -e\n",
        "\n",
        "cd \"$(dirname \"${BASH_SOURCE[0]}\")\"\n",
        "\n",
        "if [[ \"$(pwd)\" =~ \" \" ]]; then\n",
        "    echo \"This script relies on Miniconda which cannot be silently installed under a path with spaces.\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "# Deactivate existing conda environments to avoid conflicts.\n",
        "{ conda deactivate && conda deactivate && conda deactivate; } 2> /dev/null\n",
        "\n",
        "OS_ARCH=$(uname -m)\n",
        "case \"${OS_ARCH}\" in\n",
        "    x86_64*)    OS_ARCH=\"x86_64\" ;;\n",
        "    arm64* | aarch64*) OS_ARCH=\"aarch64\" ;;\n",
        "    *)          echo \"Unknown system architecture: $OS_ARCH! This script runs only on x86_64 or arm64\" && exit 1 ;;\n",
        "esac\n",
        "\n",
        "# Enable universe repository and install Linux requirements.\n",
        "sudo add-apt-repository universe\n",
        "sudo apt update\n",
        "sudo apt install -y libaio-dev espeak-ng ffmpeg gcc g++\n",
        "\n",
        "# Configuration paths.\n",
        "INSTALL_DIR=\"$(pwd)/xttsv2\"\n",
        "CONDA_ROOT_PREFIX=\"${INSTALL_DIR}/conda\"\n",
        "INSTALL_ENV_DIR=\"${INSTALL_DIR}/env\"\n",
        "MINICONDA_DOWNLOAD_URL=\"https://repo.anaconda.com/miniconda/Miniconda3-py311_24.4.0-0-Linux-${OS_ARCH}.sh\"\n",
        "\n",
        "if [ ! -x \"${CONDA_ROOT_PREFIX}/bin/conda\" ]; then\n",
        "    echo \"Downloading Miniconda from ${MINICONDA_DOWNLOAD_URL} to ${INSTALL_DIR}/miniconda_installer.sh\"\n",
        "    mkdir -p \"${INSTALL_DIR}\"\n",
        "    curl -L \"${MINICONDA_DOWNLOAD_URL}\" -o \"${INSTALL_DIR}/miniconda_installer.sh\"\n",
        "    chmod +x \"${INSTALL_DIR}/miniconda_installer.sh\"\n",
        "    bash \"${INSTALL_DIR}/miniconda_installer.sh\" -b -p \"${CONDA_ROOT_PREFIX}\"\n",
        "    echo \"Miniconda installed.\"\n",
        "fi\n",
        "\n",
        "if [ ! -d \"${INSTALL_ENV_DIR}\" ]; then\n",
        "    \"${CONDA_ROOT_PREFIX}/bin/conda\" create -y --prefix \"${INSTALL_ENV_DIR}\" -c conda-forge python=3.11.9\n",
        "    echo \"Conda environment created at ${INSTALL_ENV_DIR}.\"\n",
        "fi\n",
        "\n",
        "# Activate the conda environment and install Python requirements.\n",
        "source \"${CONDA_ROOT_PREFIX}/etc/profile.d/conda.sh\"\n",
        "conda activate \"${INSTALL_ENV_DIR}\"\n",
        "\n",
        "pip install torch==2.3.0+cu121 torchvision==0.18.0+cu121 torchaudio==2.3.0+cu121 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "conda install -y nvidia/label/cuda-12.1.0::cuda-toolkit=12.1\n",
        "conda install -y pytorch::faiss-cpu\n",
        "conda install -y -c conda-forge \"ffmpeg=*=*gpl*\"\n",
        "conda install -y -c conda-forge \"ffmpeg=*=h*_*\" --no-deps\n",
        "echo \"Fixing Nvidia's broken symlinks in the env/lib folder...\"\n",
        "env_path=\"${INSTALL_ENV_DIR}/lib\"\n",
        "echo \"Installing additional requirements...\"\n",
        "pip install coqui-tts[all]\n",
        "echo \"Installing DeepSpeed...\"\n",
        "wget https://github.com/erew123/alltalk_tts/releases/download/DeepSpeed-14.2-Linux/deepspeed-0.14.2+cu121torch2.3-cp311-cp311-manylinux_2_24_x86_64.whl\n",
        "pip install deepspeed-0.14.2+cu121torch2.3-cp311-cp311-manylinux_2_24_x86_64.whl\n",
        "conda clean --all --force-pkgs-dirs -y\n",
        "\n",
        "cd \"$(dirname \"${BASH_SOURCE[0]}\")\"\n",
        "if [[ \"$(pwd)\" =~ \" \" ]]; then\n",
        "    echo \"This script relies on Miniconda which cannot be silently installed under a path with spaces.\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "{ conda deactivate && conda deactivate && conda deactivate; } 2> /dev/null\n",
        "\n",
        "CONDA_ROOT_PREFIX=\"$(pwd)/xttsv2/conda\"\n",
        "INSTALL_ENV_DIR=\"$(pwd)/xttsv2/env\"\n",
        "export PYTHONNOUSERSITE=1\n",
        "unset PYTHONPATH\n",
        "unset PYTHONHOME\n",
        "export CUDA_PATH=\"$INSTALL_ENV_DIR\"\n",
        "export CUDA_HOME=\"$CUDA_PATH\"\n",
        "\n",
        "bash --init-file <(echo \"source \\\"$CONDA_ROOT_PREFIX/etc/profile.d/conda.sh\\\" && conda activate \\\"$INSTALL_ENV_DIR\\\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Download MODEL\n",
        "\n",
        "# Create a directory for the XTTS-v2 model files and move into it.\n",
        "!mkdir -p xtts2_model\n",
        "!wget https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth -O /content/xtts2_model/model.pth\n",
        "!wget https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/config.json -O /content/xtts2_model/config.json\n",
        "!wget https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json -O /content/xtts2_model/vocab.json\n",
        "!wget https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/hash.md5 -O /content/xtts2_model/hash.md5\n",
        "!wget https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/speakers_xtts.pth -O /content/xtts2_model/speakers_xtts.pth\n",
        "\n",
        "# CLONE\n",
        "!wget https://raw.githubusercontent.com/Boga28/Google-Colab/refs/heads/main/Christopher_lee_clean.wav -O /content/Christopher_lee_clean.wav\n",
        "!wget https://github.com/Boga28/Google-Colab/raw/refs/heads/main/eTTs.py -O /content/eTTs.py\n",
        "!wget https://github.com/Boga28/Google-Colab/raw/refs/heads/main/input.txt -O /content/input.txt\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0rNf-S6-AhLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Run Script\n",
        "\n",
        "%%bash\n",
        "# (Optional) Deactivate any active environments.\n",
        "{ conda deactivate && conda deactivate && conda deactivate; } 2> /dev/null\n",
        "\n",
        "# Define paths.\n",
        "CONDA_ROOT_PREFIX=\"/content/xttsv2/conda/\"\n",
        "INSTALL_ENV_DIR=\"/content/xttsv2/env\"\n",
        "\n",
        "# Export necessary variables.\n",
        "export PYTHONNOUSERSITE=1\n",
        "unset PYTHONPATH\n",
        "unset PYTHONHOME\n",
        "export CUDA_PATH=\"$INSTALL_ENV_DIR\"\n",
        "export CUDA_HOME=\"$CUDA_PATH\"\n",
        "export PATH=$CUDA_HOME/bin:$PATH\n",
        "\n",
        "# Set a valid matplotlib backend.\n",
        "export MPLBACKEND=agg\n",
        "\n",
        "# Source the conda setup script and activate the environment.\n",
        "source \"$CONDA_ROOT_PREFIX/etc/profile.d/conda.sh\"\n",
        "conda activate \"$INSTALL_ENV_DIR\"\n",
        "#pip install coqui-tts[all]\n",
        "#pip install /content/deepspeed-0.14.2+cu121torch2.3-cp311-cp311-manylinux_2_24_x86_64.whl\n",
        "\n",
        "# Run your Python script.\n",
        "python /content/eTTs.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bzXzlHD7j0u",
        "outputId": "2b75414a-f114-4ec3-e583-73cd49f12fdf"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: activate-binutils_linux-64.sh made the following environmental changes:\n",
            "+ADDR2LINE=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-addr2line\n",
            "+AR=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-ar\n",
            "+AS=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-as\n",
            "+CXXFILT=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-c++filt\n",
            "+ELFEDIT=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-elfedit\n",
            "+GPROF=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-gprof\n",
            "+HOST=x86_64-conda_cos6-linux-gnu\n",
            "+LD=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-ld\n",
            "+LD_GOLD=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-ld.gold\n",
            "+NM=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-nm\n",
            "+OBJCOPY=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-objcopy\n",
            "+OBJDUMP=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-objdump\n",
            "+RANLIB=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-ranlib\n",
            "+READELF=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-readelf\n",
            "+SIZE=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-size\n",
            "+STRINGS=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-strings\n",
            "+STRIP=/content/xttsv2/env/bin/x86_64-conda_cos6-linux-gnu-strip\n",
            "Loading model...\n",
            "[2025-02-27 12:43:55,358] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n",
            "[2025-02-27 12:43:58,064] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.2+cu121torch2.3, git-hash=unknown, git-branch=unknown\n",
            "[2025-02-27 12:43:58,065] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2025-02-27 12:43:58,066] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
            "[2025-02-27 12:43:58,066] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
            "[2025-02-27 12:43:59,186] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n",
            "Computing speaker latents...\n",
            "Running inference in chunks...\n",
            "Text split into 33 chunks.\n",
            "Chunk 1: THE VIRGINIAN\n",
            "by Owen WISTER\n",
            "\n",
            "CHAPTER I\n",
            "Enter the Man\n",
            "Some notable sight was drawing the passengers, both men and women, to the window; and therefore I rose and crossed the car to see what it was.\n",
            "Chunk 2: I saw near the track an enclosure, and round it some laughing men, and inside it some whirling dust, and amid the dust some horses, plunging, huddling, and dodging.\n",
            "Chunk 3: They were cow ponies in a corral, and one of them would not be caught, no matter who threw the rope.\n",
            "Chunk 4: We had plenty of time to watch this sport, for our train had stopped that the engine might take water at the tank before it pulled us up beside the station platform of Medicine Bow.\n",
            "Chunk 5: We were also six hours late, and starving for entertainment.\n",
            "Chunk 6: The pony in the corral was wise, and rapid of limb.\n",
            "Chunk 7: Have you seen a skilful boxer watch his antagonist with a quiet, incessant eye?\n",
            "Chunk 8: Such an eye as this did the pony keep upon whatever man took the rope.\n",
            "Chunk 9: The man might pretend to look at the weather, which was fine; or he might affect earnest conversation with a bystander: it was bootless.\n",
            "Chunk 10: The pony saw through it.\n",
            "Chunk 11: No feint hoodwinked him.\n",
            "Chunk 12: This animal was thoroughly a man of the world.\n",
            "Chunk 13: His undistracted eye stayed fixed upon the dissembling foe, and the gravity of his horse-expression made the matter one of high comedy.\n",
            "Chunk 14: Then the rope would sail out at him, but he was already elsewhere; and if horses laugh, gaiety must have abounded in that corral.\n",
            "Chunk 15: Sometimes the pony took a turn alone; next he had slid in a flash among his brothers, and the whole of them like a school of playful fish whipped round the corral, kicking up the fine dust, and (I take it) roaring with laughter.\n",
            "Chunk 16: Through the window-glass of our Pullman the thud of their mischievous hoofs reached us, and the strong, humorous curses of the cowboys.\n",
            "Chunk 17: Then for the first time I noticed a man who sat on the high gate of the corral, looking on.\n",
            "Chunk 18: For he now climbed down with the undulations of a tiger, smooth and easy, as if his muscles flowed beneath his skin.\n",
            "Chunk 19: The others had all visibly whirled the rope, some of them even shoulder high.\n",
            "Chunk 20: I did not see his arm lift or move.\n",
            "Chunk 21: He appeared to hold the rope down low, by his leg.\n",
            "Chunk 22: But like a sudden snake I saw the noose go out its length and fall true; and the thing was done.\n",
            "Chunk 23: As the captured pony walked in with a sweet, church-door expression, our train moved slowly on to the station, and a passenger remarked, “That man knows his business.” But the passenger’s dissertation upon roping I was obliged to lose, for Medicine\n",
            "Chunk 24: Bow was my station.\n",
            "Chunk 25: I bade my fellow-travellers goodbye, and descended, a stranger, into the great cattle land.\n",
            "Chunk 26: And here in less than ten minutes I learned news which made me feel a stranger indeed.\n",
            "Chunk 27: My baggage was lost; it had not come on my train; it was adrift somewhere back in the two thousand miles that lay behind me.\n",
            "Chunk 28: And by way of comfort, the baggage-man remarked that passengers often got astray from their trunks, but the trunks mostly found them after a while.\n",
            "Chunk 29: Having offered me this encouragement, he turned whistling to his affairs and left me planted in the baggage-room at Medicine Bow.\n",
            "Chunk 30: I stood deserted among crates and boxes, blankly holding my check, hungry and forlorn.\n",
            "Chunk 31: I stared out through the door at the sky and the plains; but I did not see the antelope shining among the sagebrush, nor the great sunset light of Wyoming.\n",
            "Chunk 32: Annoyance blinded my eyes to all things save my grievance: I saw only a lost trunk.\n",
            "Chunk 33: And I was muttering half-aloud, “What a forsaken hole this is!” when suddenly from outside on the platform came a slow voice:⁠—\n",
            "Chunk 1/33 synthesized in 4.81s\n",
            "Chunk 2/33 synthesized in 4.17s\n",
            "Chunk 3/33 synthesized in 2.15s\n",
            "Chunk 4/33 synthesized in 3.83s\n",
            "Chunk 5/33 synthesized in 1.52s\n",
            "Chunk 6/33 synthesized in 1.59s\n",
            "Chunk 7/33 synthesized in 2.01s\n",
            "Chunk 8/33 synthesized in 1.25s\n",
            "Chunk 9/33 synthesized in 2.96s\n",
            "Chunk 10/33 synthesized in 0.41s\n",
            "Chunk 11/33 synthesized in 1.09s\n",
            "Chunk 12/33 synthesized in 1.22s\n",
            "Chunk 13/33 synthesized in 2.49s\n",
            "Chunk 14/33 synthesized in 2.78s\n",
            "Chunk 15/33 synthesized in 5.46s\n",
            "Chunk 16/33 synthesized in 3.12s\n",
            "Chunk 17/33 synthesized in 2.41s\n",
            "Chunk 18/33 synthesized in 2.79s\n",
            "Chunk 19/33 synthesized in 1.50s\n",
            "Chunk 20/33 synthesized in 0.95s\n",
            "Chunk 21/33 synthesized in 1.66s\n",
            "Chunk 22/33 synthesized in 2.53s\n",
            "Chunk 23/33 synthesized in 5.09s\n",
            "Chunk 24/33 synthesized in 1.03s\n",
            "Chunk 25/33 synthesized in 2.18s\n",
            "Chunk 26/33 synthesized in 3.42s\n",
            "Chunk 27/33 synthesized in 2.60s\n",
            "Chunk 28/33 synthesized in 3.69s\n",
            "Chunk 29/33 synthesized in 2.27s\n",
            "Chunk 30/33 synthesized in 2.06s\n",
            "Chunk 31/33 synthesized in 3.95s\n",
            "Chunk 32/33 synthesized in 1.93s\n",
            "Chunk 33/33 synthesized in 2.60s\n",
            "Total synthesis time: 83.52s\n",
            "Final audio saved as xtts.m4a\n",
            "------------------------------------------------------\n",
            "Free memory : 12.554810 (GigaBytes)  \n",
            "Total memory: 14.741272 (GigaBytes)  \n",
            "Requested memory: 0.335938 (GigaBytes) \n",
            "Setting maximum total tokens (input + output) to 1024 \n",
            "WorkSpace: 0x123de2000000 \n",
            "------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VRoqK6o9vMC0"
      }
    }
  ]
}